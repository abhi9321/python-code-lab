{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Spark DataFrame?\n",
    "- A **DataFrame** is a distributed collection of data organized into named columns, similar to a table in a relational database.\n",
    "- It is built on top of **RDDs (Resilient Distributed Datasets)** and provides a higher-level abstraction.\n",
    "- It is Immutable, we always create a new DataFrame from existing DataFrame.\n",
    "\n",
    "### Key Features\n",
    "- **Schema**: DataFrames have a schema that defines the structure of the data, including column names and types.\n",
    "- **Lazy Evaluation**: Transformations on DataFrames are lazily evaluated, meaning they are not computed until an action (like `show()` or `collect()`) is called.\n",
    "- **Optimized Execution**: Spark uses a query optimizer to optimize the execution plan.\n",
    "\n",
    "### Common Operations\n",
    "- **Creating DataFrames**:\n",
    "  ```python\n",
    "  from pyspark.sql import SparkSession\n",
    "  spark = SparkSession.builder.getOrCreate()\n",
    "  df = spark.createDataFrame(data, schema)\n",
    "  ```\n",
    "  \n",
    "### Spark Fundamental Operations\n",
    "- **Transformation**\n",
    "- **Action**\n",
    "\n",
    "### Transformations\n",
    "Transformations create a new DataFrame from an existing one. They are **lazy**, meaning they are not executed until an action is called.\n",
    "\n",
    "1. **`select(*cols)`**: Selects specific columns from the DataFrame.\n",
    "   ```python\n",
    "   df.select(\"column1\", \"column2\").show()\n",
    "   ```\n",
    "\n",
    "2. **`filter(condition)`**: Filters rows based on a specified condition.\n",
    "   ```python\n",
    "   df.filter(df.age > 21).show()\n",
    "   ```\n",
    "\n",
    "3. **`withColumn(colName, expr)`**: Adds a new column or replaces an existing one.\n",
    "   ```python\n",
    "   df.withColumn(\"new_col\", df.age + 1).show()\n",
    "   ```\n",
    "\n",
    "4. **`drop(*cols)`**: Removes specified columns from the DataFrame.\n",
    "   ```python\n",
    "   df.drop(\"column1\").show()\n",
    "   ```\n",
    "\n",
    "5. **`groupBy(*cols)`**: Groups the DataFrame using specified columns for aggregation.\n",
    "   ```python\n",
    "   df.groupBy(\"column1\").count().show()\n",
    "   ```\n",
    "\n",
    "6. **`join(other, on, how)`**: Joins two DataFrames based on a common column.\n",
    "   ```python\n",
    "   df1.join(df2, \"id\", \"inner\").show()\n",
    "   ```\n",
    "\n",
    "7. **`distinct()`**: Returns a new DataFrame with distinct rows.\n",
    "   ```python\n",
    "   df.distinct().show()\n",
    "   ```\n",
    "\n",
    "8. **`orderBy(*cols)`**: Sorts the DataFrame by specified columns.\n",
    "   ```python\n",
    "   df.orderBy(\"column1\").show()\n",
    "   ```\n",
    "\n",
    "9. **`union(other)`**: Combines two DataFrames with the same schema.\n",
    "   ```python\n",
    "   df1.union(df2).show()\n",
    "   ```\n",
    "\n",
    "10. **`sample(withReplacement, fraction)`**: Returns a sampled subset of the DataFrame.\n",
    "    ```python\n",
    "    df.sample(False, 0.1).show()\n",
    "    ```\n",
    "\n",
    "10. **`Coalesce()`**: Reduces the number of partitions in a DataFrame without shuffling data.\n",
    "    ```python\n",
    "    reduced_df = df.coalesce(2)  # Reduces to 2 partitions\n",
    "    print(f\"Number of partitions after coalesce: {reduced_df.rdd.getNumPartitions()}\")\n",
    "    ```\n",
    "\n",
    "11. **`Repartition()`**: Reduces the number of partitions in a DataFrame without shuffling data.\n",
    "    ```python\n",
    "    increased_df = df.repartition(4)  # Increases to 4 partitions\n",
    "    print(f\"Number of partitions after repartition: {increased_df.rdd.getNumPartitions()}\")\n",
    "    ```\n",
    "\n",
    "There are 2 types of transformations:\n",
    "   - **`Narrow`** transformation\n",
    "   - **`wide`** transformation\n",
    "\n",
    "### 1. Narrow Transformations\n",
    "These transformations do not require data to be shuffled across partitions.\n",
    "\n",
    "- **map()**: Applies a function to each element of the RDD.\n",
    "  ```python\n",
    "  from pyspark.sql import SparkSession\n",
    "\n",
    "  spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "  rdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "  result = rdd.map(lambda x: x * 2).collect()\n",
    "  print(result)\n",
    "  # Output: [2, 4, 6, 8]\n",
    "  ```\n",
    "\n",
    "- **filter()**: Returns a new RDD containing only the elements that satisfy a given condition.\n",
    "  ```python\n",
    "  result = rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "  print(result)\n",
    "  # Output: [2, 4]\n",
    "  ```\n",
    "\n",
    "- **flatMap()**: Similar to map, but each input item can be mapped to zero or more output items (flattening the results).\n",
    "  ```python\n",
    "  rdd = spark.sparkContext.parallelize([\"hello world\", \"apache spark\"])\n",
    "  result = rdd.flatMap(lambda line: line.split(\" \")).collect()\n",
    "  print(result)\n",
    "  # Output: ['hello', 'world', 'apache', 'spark']\n",
    "  ```\n",
    "\n",
    "### 2. Wide Transformations\n",
    "These transformations involve shuffling data across partitions.\n",
    "\n",
    "- **groupByKey()**: Groups the data by key.\n",
    "  ```python\n",
    "  rdd = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "  result = rdd.groupByKey().mapValues(list).collect()\n",
    "  print(result)\n",
    "  # Output: [('a', [1, 3]), ('b', )]\n",
    "  ```\n",
    "\n",
    "- **reduceByKey()**: Combines values with the same key using a specified associative function.\n",
    "  ```python\n",
    "  result = rdd.reduceByKey(lambda x, y: x + y).collect()\n",
    "  print(result)\n",
    "  # Output: [('a', 4), ('b', 2)]\n",
    "  ```\n",
    "\n",
    "- **join()**: Joins two RDDs by their keys.\n",
    "  ```python\n",
    "  rdd1 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "  rdd2 = spark.sparkContext.parallelize([(\"a\", 3), (\"b\", 4)])\n",
    "  result = rdd1.join(rdd2).collect()\n",
    "  print(result)\n",
    "  # Output: [('a', (1, 3)), ('b', (2, 4))]\n",
    "  ```\n",
    "\n",
    "### Actions\n",
    "Actions trigger the execution of transformations and return a result to the driver program.\n",
    "\n",
    "1. **`show(n)`**: Displays the first `n` rows of the DataFrame.\n",
    "   ```python\n",
    "   df.show(5) # when you want a quick visual overview of the data\n",
    "   ```\n",
    "\n",
    "2. **`head(n)`**: Returns the first `n` rows of the DataFrame as a list of Row.\n",
    "   ```python\n",
    "   df.head(5) # when you need to work with the data programmatically after retrieving it\n",
    "   ```\n",
    "\n",
    "3. **`count()`**: Returns the number of rows in the DataFrame.\n",
    "   ```python\n",
    "   df.count()\n",
    "   ```\n",
    "\n",
    "4. **`collect()`**: Returns all rows as a list to the driver.\n",
    "   ```python\n",
    "   data = df.collect()\n",
    "   ```\n",
    "\n",
    "5. **`first()`**: Returns the first row of the DataFrame.\n",
    "   ```python\n",
    "   first_row = df.first()\n",
    "   ```\n",
    "\n",
    "6. **`take(n)`**: Returns the first `n` rows as a list.\n",
    "   ```python\n",
    "   rows = df.take(5)\n",
    "   ```\n",
    "\n",
    "7. **`describe(*cols)`**: Provides summary statistics for numerical columns.\n",
    "   ```python\n",
    "   df.describe().show()\n",
    "   ```\n",
    "\n",
    "8. **`printSchema()`**: Displays the schema of the DataFrame.\n",
    "   ```python\n",
    "   df.printSchema()\n",
    "   ```\n",
    "\n",
    "9. **`read`**: Read data from various sources like CSV, JSON, Parquet, and more\n",
    "   ```python\n",
    "   df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", \"true\") \\\n",
    "      .option(\"inferSchema\", \"true\") \\\n",
    "      .load(\"/path/to/file.csv\")\n",
    "   df.show()\n",
    "   ```\n",
    "\n",
    "10. **`write.format(\"format\").save(path)`**: Saves the DataFrame to a specified format (e.g., CSV, Parquet).\n",
    "   ```python\n",
    "   df.write.format(\"csv\").save(\"output.csv\")\n",
    "   ```\n",
    "\n",
    "\n",
    "### Summary\n",
    "- **Transformations** are used to create new DataFrames and are executed lazily.\n",
    "- **Actions** trigger the execution of transformations and return results.\n",
    "\n",
    "\n",
    "### Useful Methods\n",
    "- **Data Inspection**:\n",
    "  - `df.printSchema()`: Prints the schema of the DataFrame.\n",
    "  - `df.columns`: Lists all column names.\n",
    "  - `df.dtypes`: Shows data types of each column.\n",
    "\n",
    "- **Handling Missing Data**:\n",
    "  - `df.dropna()`: Removes rows with null values.\n",
    "  - `df.fillna(value)`: Replaces null values with a specified value.\n",
    "\n",
    "### Example\n",
    "Here's a simple example of creating and manipulating a DataFrame:\n",
    "```python\n",
    "data = [(1, \"Alice\"), (2, \"Bob\")]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "df.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Execution Methods\n",
    "\n",
    "#### Interactive Clients\n",
    "- **`spark-shell`**: This is a powerful interactive shell that allows you to run Spark commands. It's great for testing and debugging your Spark applications.\n",
    "  ```bash\n",
    "    pyspark --master yarn --driver-memory 1G --executor-memory 1G --num-executors 2 --executor-cores 2\n",
    "  ```\n",
    "- **`Notebooks`**: Tools like Jupyter Notebooks or Apache Zeppelin provide an interactive environment where you can write and execute Spark code in various languages (Scala, Python, R). These are particularly useful for data exploration and visualization.\n",
    "\n",
    "#### Submit Job\n",
    "- **`spark-submit`**: This is the command-line tool used to submit Spark applications to a cluster. It allows you to run your application in a distributed environment, specifying various configurations like the master URL, application name, and resource allocation.\n",
    "Here's a breakdown of its key components and usage:\n",
    "\n",
    "### Basic Syntax\n",
    "```bash\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  --driver-memory <value> \\\n",
    "  --executor-memory <value> \\\n",
    "  --executor-cores <number> \\\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```\n",
    "\n",
    "### Key Options\n",
    "- **--class**: The entry point for your application (e.g., `org.apache.spark.examples.SparkPi`).\n",
    "- **--master**: The master URL for the cluster (e.g., `spark://23.195.26.187:7077`)(Default: local[*]).\n",
    "- **--deploy-mode**: Specifies whether to deploy your driver on the worker nodes (`cluster`) or locally as an external client (`client`).\n",
    "- **--conf**: Arbitrary Spark configuration properties in `key=value` format.\n",
    "- **--driver-memory**: Amount of memory to use for the driver process (e.g., `4g`, Default `1g`).\n",
    "- **--executor-memory**: Amount of memory to use per executor process (e.g., `2g`).\n",
    "- **--executor-cores**: Number of cores to use on each executor.\n",
    "- **<application-jar>**: Path to the bundled jar including your application and all dependencies.\n",
    "- **[application-arguments]**: Arguments passed to the main method of your main class, if any.\n",
    "\n",
    "### Deployment Modes\n",
    "- **`Client Mode`**: The driver runs on the machine where `spark-submit` is executed. This mode is suitable for interactive applications.\n",
    "- **`Cluster Mode`**: The driver runs on one of the worker nodes in the cluster. This mode is suitable for production jobs.\n",
    "\n",
    "### Example Usage\n",
    "```bash\n",
    "./bin/spark-submit \\\n",
    "  --class org.apache.spark.examples.SparkPi \\\n",
    "  --master spark://23.195.26.187:7077 \\\n",
    "  --deploy-mode cluster \\\n",
    "  --driver-memory 4g \\\n",
    "  --executor-memory 2g \\\n",
    "  --executor-cores 2 \\\n",
    "  /path/to/your-application.jar \\\n",
    "  1000\n",
    "```\n",
    "\n",
    "This command submits a Spark application that calculates Pi using the `SparkPi` example class. The application runs in cluster mode with specified memory and core configurations(https://spark.apache.org/docs/latest/submitting-applications.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Distributed Processing Model\n",
    "Once a Spark job is submitted, several key steps occur to ensure the job is executed efficiently across the cluster. Here's a detailed breakdown of the process:\n",
    "\n",
    "### 1. Job Submission\n",
    "- **Driver Program**: The `spark-submit` command launches the driver program, which is the main entry point of your Spark application. The driver is responsible for converting the user code into a logical execution plan and requesting resources from the cluster manager.\n",
    "\n",
    "### 2. Resource Allocation\n",
    "- **Cluster Manager**: The driver communicates with the cluster manager (e.g., YARN, Mesos, Kubernetes, or Standalone) to request resources. The cluster manager allocates resources (CPU, memory) for the executors on the worker nodes\n",
    "\n",
    "### 3. Task Scheduling\n",
    "- **DAG Scheduler**: The driver converts the user code into a logical execution plan, whihc is represented as a Directed Acyclic Graph (DAG) of stages based on the transformations and actions in the job. The DAG scheduler divides the job into stages and submits them to the task scheduler(https://sparkbyexamples.com/spark/what-is-spark-stage/).\n",
    "- **Task Scheduler**: The task scheduler assigns tasks to executors based on data locality and resource availability. It ensures tasks are executed efficiently across the cluster(https://spark.apache.org/docs/latest/job-scheduling.html).\n",
    "\n",
    "### 4. Task Execution\n",
    "- **Executors**: Executors are launched on the worker nodes to run the tasks. They perform the computations and store intermediate data in memory or on disk. Executors communicate with the driver to report the status of tasks and data.\n",
    "\n",
    "### 5. Result Collection\n",
    "- **Driver Program**: The driver collects the results from the executors. Depending on the action performed (e.g., `collect`, `count`), the results are either returned to the driver or written to an external storage system.\n",
    "\n",
    "### 6. Job Completion\n",
    "- **Resource Cleanup**: Once the job is completed, the driver program terminates, and the resources allocated by the cluster manager are released. Executors are shut down, and any temporary data stored on the worker nodes is cleaned up\n",
    "\n",
    "### Example Workflow\n",
    "1. **Submit Job**: User submits the job using `spark-submit`.\n",
    "2. **Driver Initialization**: Driver program starts and requests resources.\n",
    "3. **DAG Creation**: Driver constructs the DAG of stages.\n",
    "4. **Task Scheduling**: Tasks are scheduled and sent to executors.\n",
    "5. **Task Execution**: Executors run the tasks and perform computations.\n",
    "6. **Result Collection**: Results are collected and returned to the driver.\n",
    "7. **Job Completion**: Resources are released, and the job is marked as complete.\n",
    "\n",
    "This process ensures that Spark can efficiently process large datasets by distributing tasks across multiple nodes and leveraging in-memory computation.\n",
    "\n",
    "\n",
    "Source:\n",
    "- The Internal Working of Apache Spark | Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/08/understand-the-internal-working-of-apache-spark/.\n",
    "- What is Spark Stage? Explained - Spark By Examples. https://sparkbyexamples.com/spark/what-is-spark-stage/.\n",
    "- Job Scheduling - Spark 3.5.2 Documentation - Apache Spark. https://spark.apache.org/docs/latest/job-scheduling.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status of Job\n",
    "\n",
    "To check the status of running jobs and view details of completed jobs in Apache Spark, you can use several tools and interfaces. Here are the main methods:\n",
    "\n",
    "### 1. Spark Web UI\n",
    "The Spark Web UI provides a comprehensive view of the status and details of Spark jobs. It includes several tabs to monitor different aspects of your Spark application:\n",
    "\n",
    "- **Jobs Tab**: Displays a summary of all jobs, including their status (running, succeeded, failed), duration, and progress. Clicking on a job provides detailed information such as the event timeline, DAG visualization, and stages of the job¹(https://spark.apache.org/docs/latest/web-ui.html).\n",
    "- **Stages Tab**: Shows the current state of all stages in the Spark application, including active, pending, completed, skipped, and failed stages.\n",
    "- **Storage Tab**: Provides information about RDDs and DataFrames that are cached.\n",
    "- **Environment Tab**: Displays Spark configuration properties.\n",
    "- **Executors Tab**: Shows details about the executors, including memory and CPU usage.\n",
    "- **SQL Tab**: Provides metrics for SQL queries if you are using Spark SQL.\n",
    "\n",
    "### 2. Spark History Server\n",
    "The Spark History Server allows you to view the details of completed Spark applications. It provides similar information to the Spark Web UI but for past jobs. You can start the History Server using the following command:\n",
    "```bash\n",
    "./sbin/start-history-server.sh\n",
    "```\n",
    "Ensure that the event logs are enabled in your Spark configuration:\n",
    "```bash\n",
    "spark.eventLog.enabled true\n",
    "spark.eventLog.dir hdfs:///path/to/event/logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32a725944b8810b16d997e1970a86d6a00642d190c49302d97ca631319c2ddf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
